---
title: "HS614 HW#2"
author: "Tiffany Chua"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries}
library(Amelia)
library(ggplot2)
library("GGally")
library(caTools)
library(caret)
library(class)
library("e1071")
library(psych)
library(corrplot)
library("ISLR")
```

##HW #2:
For this HW do more exploratory data analysis (EDA) like adding some graphs, correlation btw variables. After that apply all the classification models we have learned so far. 

```{r Dataset}
# https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(original)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"
names = c('id', 'clump_thickness', 'uniform_cell_size', 'uniform_cell_shape',
       'marginal_adhesion', 'single_epithelial_size', 'bare_nuclei',
       'bland_chromatin', 'normal_nucleoli', 'mitoses', 'class')
df = read.csv(url, col.names=names)

df
```

Attribute Domain
1. Sample code number id number
2. Clump Thickness 1 - 10
3. Uniformity of Cell Size 1 - 10
4. Uniformity of Cell Shape 1 - 10
5. Marginal Adhesion 1 - 10
6. Single Epithelial Cell Size 1 - 10
7. Bare Nuclei 1 - 10
8. Bland Chromatin 1 - 10
9. Normal Nucleoli 1 - 10
  A. Mitoses 1 - 10
  B. Class: (2 for benign, 4 for malignant)
  
  
```{r}
str(df)
```
  
```{r}
#Change bare_nuclei to int
df$bare_nuclei <- as.integer(df$bare_nuclei)

#Change class to factor
df$class = factor(df$class,
                  levels = c(2, 4),
                  label = c(0, 1))

str(df)
```
  
```{r}
nrow(df)
ncol(df)
```

```{r}
table(df$bare_nuclei)
```

```{r}
df = subset(df,
            df$bare_nuclei != "?")

table(df$bare_nuclei)
```

```{r}
str(df)

sum(is.na(df))
```

```{r}
df = df[ , -1]
str(df)
```
  
```{r}
summary(df)
```

```{r}
missmap(df,
        main = "Missing Map",
        col = c("blue",
                "red"))
```

```{r}
ggpairs(df)
```

```{r}
#Check how many in each class
table(df$class)
```

###Plots
```{r Factor Bar}
ggplot(df,
       aes(class)) + 
  geom_bar(aes(fill = factor(class)))
```

```{r Boxplots}
ggplot(df,
       aes(class, clump_thickness)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, uniform_cell_size)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, uniform_cell_shape)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, marginal_adhesion)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, single_epithelial_size)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, bare_nuclei)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, bland_chromatin)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, normal_nucleoli)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))

ggplot(df,
       aes(class, mitoses)) + 
  geom_boxplot(aes(group = class,
                   fill = factor(class)))
```

##Model
```{r}
#Train and test split
#Train 2 models:
  #1. KNN
  #2. Logistic Regression
  #3. SVM
```

```{r Sample Split}
# train test split
set.seed(123)
split <- sample.split(df$class,
                      SplitRatio = 0.80)
df.train <- subset(df,
                   split == TRUE)
df.test <- subset(subset(df,
                         split == FALSE))

nrow(df.test)
nrow(df.train)
```

```{r}
str(df.train)
```

##K-NN Model
```{r Feature Scaling}
#Feature Scaling
normParam <- preProcess(df.train,
                        method = c("center",
                                   "scale"))
df.train1 <- predict(normParam,
                        df.train)
df.test1 <- predict(normParam, df.test)

head(df.train1)
head(df.test1)
```

```{r KNN}
# Fitting K-NN to the Training set and Predicting the Test set results
y_pred = knn(train = df.train1,
             test = df.test1,
             df.train1$class,
             k = 9)

head(y_pred)

misclass.error = mean(df.test1$class != y_pred)
misclass.error

#Misclass error with optimal K value of 9 = 0.02189781
```

```{r Choosing a K}
#Choosing a K-value
y_pred <- NULL
error.rate <- NULL

for (i in 1:20){
  set.seed(123)
  y_pred <- knn(train = df.train1,
                test = df.test1,
                df.train1$class,
                k = i)
  error.rate[i] <- mean(df.test1$class != y_pred)
}

error.rate

#Visualize K - Elbow Method

k.values <- 1:20
error.df <- data.frame(error.rate,
                       k.values)

ggplot(error.df,
       aes(k.values,
           error.rate)) +
  geom_point() +
  geom_line(lty = "dotted",
            color = "red")

#Optimal K value: 9
```

```{r KNN Confusion Matrix}
cm = table(df.test1$class,
           y_pred)
cm

confusionMatrix(df.test1$class,
                y_pred)

#Accuracy: 0.9708
#Kappa: 0.9359
```

##Log Models
```{r Log Model 1}
log.model <- glm(formula=class ~ . ,
                 family=binomial,
                 data = df.train)

summary(log.model)
#Significant features:
# clump_thickness   ***
# marginal_adhesion .
# bare_nuclei       ***
# bland_chromatin   **
# normal_nucleoli   *

#AIC: 89.506
```

```{r Log Model 2}
log.model2 <- glm(formula=class ~ clump_thickness + marginal_adhesion + bare_nuclei + bland_chromatin + normal_nucleoli,
                 family=binomial,
                 data = df.train)

summary(log.model2)

#New AIC: 88.222 --> Better
```

```{r Log Model 3}
log.model3 <- glm(formula=class ~ clump_thickness +  bare_nuclei + bland_chromatin + normal_nucleoli,
                 family=binomial,
                 data = df.train)

summary(log.model3)

#New AIC: 93.302 --> Not as good as model or model2
```

